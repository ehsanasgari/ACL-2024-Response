Thank you for your valuable feedback and for recognizing the timeliness and significance of our work on parameter-efficient fine-tuning in NLP. We are committed to addressing the concerns raised to improve our manuscript.

**Reproducibility Concerns**
Acknowledging the reproducibility score of 3, we aim to clarify any ambiguity regarding parameter settings and ensure our methods and datasets are comprehensively detailed for ease of replication.

**Clarity on Experimental Results and Superposition Effectiveness**
Regarding the effectiveness and stability improvement of our method through employing multiple token embeddings, it's pertinent to mention that the case of \(m=1\) is equivalent to Simple Prompt Tuning, which is already included in Table 1 as a baseline comparison. To provide a clearer perspective, we will include these results in the corresponding figures for direct visual comparison. The omission was an oversight, and we appreciate your pointing it out.

**Discussion on Dropout and Convergence Speed**
The accelerated convergence observed in most tasks is partially masked by the choice of evaluation metric. For instance, CoLA utilizes the Matthews Correlation Coefficient (MCC), where outputting the most frequent class results in a score of zero but with accuracy it would achieve the score coresspond to frequency of the the most frequent class. According to Figure 2.(a), the model with dropout in MRPC started to generalize after epoch 25, and in RTE, the model with dropout started to generalize after epoch 50. These instances highlight the negative effect of dropout, delaying the model's ability to generalize in the training process.

**Typos and Language Corrections**
We thank you for identifying the typo in line 153 and will ensure it is corrected in our revised manuscript.

We are grateful for the opportunity to refine our work and believe these revisions will strengthen our contribution to the field. Thank you again for your constructive critique.
