Thank you for your detailed review and constructive feedback. Your insights have been invaluable in guiding our revisions to enhance the clarity and impact of our work.

**Reproducibility Concerns**
Acknowledging the reproducibility score of 2, we commit to providing more detailed descriptions of our experimental setup and ensuring that all used data and base models are publicly available. We will also release the training code upon publication to aid in replicability.

**Response to Weaknesses**

**Generalization of Experiments**: We acknowledge your concern regarding the focus on NLU tasks. It's indeed common in PEFT research to benchmark on NLU tasks due to their prevalent use as standard benchmarks. However, we recognize the importance of diversifying evaluation contexts, including generative tasks, to fully assess our method's versatility. We plan to address this in future work.

**Table 2 Clarification**: The bold values in Table 2 highlight the lowest standard deviation across tasks, indicating higher stability. We realize this may have been unclear and will clarify this criterion in the revised manuscript.

**Hyperparameter Selection**: As we mentioned, because we didn’t have the resource to do hyperparameter tuning on all methods we used hyperparameters suggested in their paper. It’s worth noting that if we consider the results reported in the original ATTEMPT paper and compare it to our method on the same base model, our results are marginally better without using any pre-trained source prompt according to Table 3 in appendix.

**Dropout Analysis**: In prompt tuning, the main network considered to be frozen and fixed. Our finding was that unlike full fine-tuning dropout doesn’t make the model better during prompt tuning in this architecture. Dropout is a hyperparameter of the main foundation model and not a hyperparameter of prompt tuning and to our knowledge, this effect rarely explored as most people assume that dropout always helps.

**Figure 1 Explanation**: We are thankful about the feedback on Figure 1 and the effectiveness of our proposed method compared to residual prompt tuning. To clarify, Figure 1 (b, e, and f) illustrates different aspects of the same method, rather than new variants. Specifically, (b) and (e) offer different visual perspectives of our method, while (f) details its linear algebra computation.

**Response to Suggestions And Typos**

**Learning Curve Stability**: We conducted additional tests to examine the behavior across different random seeds, particularly with the T5-lm-adapt model, and observed consistent patterns. This will be elaborated upon in the revised manuscript.

**Experiment Repetitions**: Each model was trained for 80 epochs, with the best-performing epoch on the validation dataset reported in Table 1. This process was consistently applied across all experiments to ensure comparability.

**Architecture Sensitivity**: Our preliminary experiments suggest SuperPos-Prompt's performance may vary across different architectural frameworks. Due to resource limitations, a comprehensive analysis across various architectures was not feasible. We acknowledge this as an area for future investigation to better understand our method's adaptability.

**Cosine Similarity Analysis**: Incorporating the cosine similarity comparison between SuperPos-Prompt and simple prompt embeddings is an excellent suggestion. We will add this analysis to our final revision, providing further insights into the embeddings' nature and our method's effectiveness.

We are grateful for the opportunity to address these points and believe the revisions will significantly strengthen the paper. Thank you once again for your thorough review and helpful suggestions.
